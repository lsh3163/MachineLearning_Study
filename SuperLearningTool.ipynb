{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Scikit-learn을 이용한 Classification\n### 1. K-NN(k-nearest neighbors)\n* 장점 : 작은 데이터셋일 경우, 기본 모델로서 좋고 설명하기 쉬움.\n* 매개변수 : n_neighbors(커지면 모델이 단순해지고 작아지면 모델이 복잡해짐)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2. 선형 모델(logistic model)\n* 장점 : 첫 번째로 시도할 알고리즘, 대용량 데이터 셋 가능. 고차원 데이터에 가능\n* 매개변수 : C가 작을수록 모델이 단순해짐(로그 스케일)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.svm import LinearSVC\nsvc = LinearSVC(C=1)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3. 결정 트리(Decision Tree)\n* 장점 : 매우 빠르고 데이터 스케일 조정이 필요 없다. 시각화하기 좋고 설명하기 쉽다.\n* 매개변수 : max_depth = 1~5(깊을수록 과대적합)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=4, random_state=0)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 4. 랜덤 포레스트(Random Forest)\n* 장점 : 결정트리 하나보다 좋은 성능을 내고 안정적이다. \n* 단점 : 고차원 희소 데이터에는 안 맞다.\n* 매개변수\n  * n_jobs : 사용할 코어의 수(-1이면 전체 사용)\n  * random_state : 고정 요망\n  * n_estimators : 클수록 좋음.(aksgdms xmflfmf vudrbsgkseksms Emt)\n  * max_features : 각 트리가 얼마나 무작위일지 결정(디뽈트 추천)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators=100, random_state=2)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 5. 그래디언트 부스팅 회귀 트리(Gradient Boosting Classifier)\n* 장점 : 랜덤 포레스트보다 조금 더 성능이 좋다. 매개변수 튜닝이 필요하다.\n* 매개변수\n  * n_estimators : 크면 모델이 복잡하고 과대적합될 가능성이 있다.\n  * learning_rate : 이전 트리의 오차를 보정하는 정도이다."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(random_state=0)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 6. 커널 서포트 벡터 머신(Kernelized support vector machines)\n* 장점 : 중간 규모 데이터셋에 잘 맞는다. 매개변수에 민감하다.\n* 매개변수\n  * gamma : 작은 값이 더 복잡한 모델을 만든다.\n  * C : 작은 값이 단순한 모델을 만든다."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.svm import SVC\nsvm = SVC(kernel='rbf', C=10, gamma=0.1)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 7. 다층 퍼셉트론(Multilayer perceptrons, MLP)\n* 장점 : 대용량 데이터셋에서 복잡한 모델을 만들 수 있다. 매개변수 선택과 데이터 스케일에 민감하며 큰 모델은 학습이 오래 걸린다.\n* 매개변수\n  * solver='lbfgs' : 최적화 알고리즘\n  * random_state\n  * hidden_layer_sizes=[100] : 기본적으로 100개(줄이면 복잡도가 낮아짐)\n  * activiation=\"tanh\" : 초평면을 부드럽게 만들어줌\n  * alpha : 모델의 복잡도 제어"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.neural_network import MLPClassifier\n# 필요하면 activiation, alpha 추가\nmlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[100])",
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}